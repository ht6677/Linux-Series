# 计算机系统架构

# CPU

简而言之，计算机由连接到内存的中央处理器（CPU）组成。下图说明了所有计算机操作背后的一般原理。

![The CPU 原理简化](https://s2.ax1x.com/2020/01/26/1nPs54.png)

CPU 对寄存器中保存的值执行指令。此示例显示首先将 R1 的值设置为 100，将值从存储器位置 0x100 加载到 R2，将这两个值加在一起并将结果放入 R3，最后将新值（110）存储到 R4 以供进一步使用。CPU 执行从内存读取的指令，指令有两大类：

- 那些将值从存储器加载到寄存器，并将值从寄存器存储到存储器的函数。

- 那些对存储在寄存器中的值进行运算的变量。例如，对两个寄存器中的值进行相加，相减或相除，执行按位运算（和，或，异或等）或执行其他数学运算（平方根，sin，cos，tan 等）。

在上面的示例中，我们仅将 100 加到存储在内存中的值，然后将此新结果存储回内存。

## 指令基础

### Branching

除了加载或存储，CPU 的另一个重要操作是分支。在内部，CPU 在指令指针中保留要执行的下一条指令的记录。通常，指令指针递增以顺序指向下一条指令。分支指令通常将检查特定寄存器是否为零或是否设置了标志，如果是，则将指针修改为另一个地址。因此，下一条要执行的指令将来自程序的不同部分。这就是循环和决策语句的工作方式。

例如，可以通过查找两个寄存器中的或来实现类似 if（x == 0）的语句，其中一个保存 x，另一个保存零。如果结果为零，则比较为真（即 x 的所有位均为零），并且应采用语句的主体，否则分支通过主体代码。

### Cycles

我们都熟悉以兆赫兹或千兆赫兹（每秒数百万或数亿个周期）给出的计算机速度。之所以称为时钟速度，是因为它是计算机内部时钟的脉动速度。

在处理器内使用脉冲以保持其内部同步。在每个滴答声或脉冲时，可以开始另一种操作； 就像时钟拍打鼓的人一样，使划船者的桨保持同步。

## Fetch, Decode, Execute, Store

执行一条指令包括一个特定的事件周期。提取，解码，执行和存储。例如，要在 CPU 上方执行添加指令，必须

- 提取：将指令从内存中获取到处理器中。

- 解码：内部解码它要做的事情（在本例中为 add）。

- 执行：从寄存器中获取值，然后将它们实际相加

- 存储：将结果存储回另一个寄存器（Retiring the instruction）。

### CPU 内部结构

在内部，CPU 具有执行上述每个步骤的许多不同子组件，通常它们可以彼此独立工作。这类似于物理生产线，那里有许多工作站，每个步骤都有特定的任务要执行。完成后，它可以将结果传递到下一个测站，并接受新的输入进行处理。

![CPU Insides](https://s2.ax1x.com/2020/01/26/1nkm8J.md.png)

您可以看到指令进入并被处理器解码。CPU 有两种主要类型的寄存器，用于整数计算的寄存器和用于浮点计算的寄存器。浮点数是一种以二进制形式用小数位表示数字的方式，并且在 CPU 中的处理方式有所不同。MMX（多媒体扩展）和 SSE（流单指令多数据）或 Altivec 寄存器类似于浮点寄存器。

寄存器文件是 CPU 内部寄存器的统称。在此之下，我们拥有真正完成所有工作的 CPU 部分。我们说过，处理器要么将一个值加载或存储到寄存器中，要么从一个寄存器加载到内存中，或者对寄存器中的值进行某些操作。

算术逻辑单元（Arithmetic Logic Unit, ALU）是 CPU 操作的核心。它获取寄存器中的值并执行 CPU 能够执行的多种操作。所有现代处理器都有许多 ALU，因此每个都可以独立工作。实际上，奔腾等处理器同时具有快速和慢速 ALU。快速的 ALU 较小（因此您可以在 CPU 上容纳更多），但只能执行最常见的操作，而慢速的 ALU 可以执行所有操作，但更大。

地址生成单元（Address Generation Unit, AGU）处理与高速缓存和主存储器的对话，以将值获取到寄存器中，以供 ALU 进行操作，并将值从寄存器中获取并返回主存储器。浮点寄存器的概念相同，但其组件使用的术语略有不同。

### Pipeling

正如我们在上面看到的，当 ALU 将寄存器加在一起时，与 AGU 将值完全写回内存完全分开，因此没有理由 CPU 不能同时执行这两个操作。我们的系统中还有多个 ALU，每个 ALU 都可以处理独立的指令。最终，CPU 可能会使用其浮点逻辑来执行一些浮点运算，而整数指令也在运行中。这个过程称为流水线，可以做到这一点的处理器称为超标量架构。所有现代处理器都是超标量的。

另一个比喻可能是将管道想象为填充大理石的软管，除非大理石是 CPU 的指令。理想情况下，您将大理石放在一端，另一端（每个时钟脉冲一个），填满管道。一旦装满，对于每一个弹子（指令），您推入所有其他弹子将移至下一个位置，一个弹子将掉出末端（结果）。

但是，分支指令会对这种模型造成严重破坏，因为它们可能会或可能不会导致执行从另一个地方开始。如果您正在流水线工作，则基本上必须猜测分支将走的路，因此您知道将哪些指令带入管道。相反，如果处理器的预测不正确，则一切正常。相反，如果处理器的预测不正确，则会浪费大量时间，必须清理管道并重新启动。此过程通常称为管道冲洗，类似于必须停止并清空软管中的所有弹珠。

### Reordering

实际上，如果 CPU 是软管，则可以自由排序软管中的弹子，只要它们以与放入它们相同的顺序弹出末端即可。我们将其称为程序顺序，因为这是在计算机程序中给出指令的顺序。

```s
1: r3 = r1 * r2
2: r4 = r2 + r3
3: r7 = r5 * r6
4: r8 = r1 + r7
```

指令 2 需要等待指令 1 完全完成才能开始。这意味着管道在等待计算值时必须停顿。类似地，指令 3 和 4 也依赖于 r7。但是，指令 2 和 3 完全没有依赖性。这意味着它们在完全独立的寄存器上运行。如果我们交换指令 2 和 3，由于处理器可以做有用的工作，而不是等待流水线完成以获得上一条指令的结果，因此可以更好地对流水线进行排序。

但是，在编写非常底层的代码时，某些指令可能需要一些有关操作顺序的安全性。我们称这种需求记忆语义。如果您需要获取语义，这意味着对于此说明，您必须确保所有先前说明的结果均已完成。如果您需要发布语义，则是说此之后的所有指令都必须查看当前结果。另一个更为严格的语义是内存屏障或内存屏障，它要求操作在继续之前已提交给内存。

在某些体系结构上，处理器可以为您保证这些语义，而在另一些体系结构上，则必须明确指定它们。尽管您可能会看到这些术语，但大多数程序员无需直接担心它们。

## CISC v RISC

划分计算机体系结构的常见方法是复杂指令集计算机（CISC）和精简指令集计算机（RISC）。在第一个示例中，我们已将值显式加载到寄存器中，执行了加法运算并将保存在另一个寄存器中的结果值存储回内存。这是 RISC 计算方法的示例-仅对寄存器中的值执行运算，并显式地将值加载到存储器中或从存储器中存储值。

CISC 方法可能只是一条指令，该指令从内存中获取值，在内部执行加法并将结果写回。这意味着指令可能需要花费很多时间，但是最终两种方法都达到了相同的目标。所有现代架构都可以看做 RISC 架构：

- 尽管 RISC 使汇编编程变得更加复杂，但是由于几乎所有程序员都使用高级语言，而将汇编代码的生成工作留给了编译器，因此其他优点胜过了这个缺点。

- 因为 RISC 处理器中的指令要简单得多，所以芯片内部有更多的寄存器空间。从内存层次结构中我们知道，寄存器是最快的内存类型，最终所有指令都必须对寄存器中保存的值执行，因此在其他条件相同的情况下，更多的寄存器将导致更高的性能。

- 由于所有指令都在同一时间执行，因此可以进行流水线操作。我们知道流水线化要求将指令流不断地输入到处理器中，因此，如果某些指令花费很长时间而另一些指令却不需要，流水线就变得很复杂，无法有效执行。

### EPIC

在本书的许多示例中都使用过的 Itanium 处理器是经过修改的架构的示例，该架构称为“显式并行指令计算”。我们已经讨论了超标度处理器如何具有在处理器的不同部分中同时运行着许多指令的流水线。显然，为使此功能正常工作，应按照可以充分利用 CPU 可用元素的顺序为处理器提供可能的指令。

传统上组织进入的指令流是硬件的工作。程序按顺序发布指令；处理器必须向前看，并尝试做出有关如何组织传入指令的决定。EPIC 背后的理论是，在更高级别上有更多可用信息，这些信息可以使这些决策比处理器更好。像当前处理器一样，分析汇编语言指令流会丢失程序员可能在原始源代码中提供的许多信息。可以将其视为研究莎士比亚戏剧和阅读相同剧本的 Cliff's Notes 版本之间的区别。两者都能为您提供相同的结果，但是原始图像具有各种额外的信息，可以设置场景并深入了解角色。

因此，订购指令的逻辑可以从处理器转移到编译器。这意味着编译器编写者需要更聪明地尝试为处理器找到最佳的代码顺序。由于处理器的许多工作已移交给编译器，因此处理器也得到了显着简化。

# 内存

## 内存层级

CPU 仅可以直接从位于处理器芯片上的高速缓存中直接获取指令和数据。必须从主系统内存（随机存取内存或 RAM）中加载缓存。但是，RAM 仅在通电时才保留其内容，因此需要存储在更永久的存储器中。

| Speed   | Memory | Description                                                                                                                                                                                                     |
| ------- | ------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Fastest | Cache  | 高速缓存是实际上嵌入在 CPU 内部的内存。高速缓存内存非常快，通常只需要访问一个周期，但是由于它直接嵌入到 CPU 中，因此内存大小是有限制的。实际上，高速缓存有几个子级别（称为 L1，L2，L3），它们的速度都略有提高。 |
|         | RAM    | 处理器的所有指令和存储地址都必须来自 RAM。尽管 RAM 速度非常快，但是 CPU 仍需要花费大量时间来访问它（称为*latency*）。RAM 存储在与主板相连的单独的专用芯片中，这意味着它比高速缓存大得多。                       |
| Slowest | Disk   | 我们都熟悉软盘或 CDROM 上的软件，并将文件保存到硬盘上。我们还熟悉程序从硬盘加载所需的长时间-具有诸如旋转磁盘和移动磁头之类的物理机制意味着磁盘是最慢的存储形式。但它们也是迄今为止最大的存储形式。              |

了解内存层次结构的重点是速度和大小之间的权衡－内存越快，内存越小：

- 空间局部性（Spatial locality）表明，块内的数据可能会一起访问。
- 时间位置（Temporal locality）表明最近使用的数据可能很快会再次使用。

这意味着，通过在实际中尽可能快地实现存储相关信息（空间）小块的快速可访问存储器（时间），可以获得好处。

## 缓存详解

缓存是 CPU 体系结构中最重要的元素之一。要编写高效的代码，开发人员需要了解其系统中的缓存如何工作。高速缓存是较慢的主系统内存的非常快的副本。高速缓存比主存储器要小得多，因为它与寄存器和处理器逻辑一起包含在处理器芯片内。从计算的角度来看，这是主要的房地产，并且其最大大小在经济和物理上都有限制。随着制造商发现越来越多的方法将越来越多的晶体管填充到芯片中，高速缓存的大小已大大增加，但是即使最大的高速缓存也只有几十兆字节，而不是主存储器的千兆字节或硬盘的数兆字节。

缓存由镜像主内存的小块组成。这些块的大小称为行大小，通常为 32 或 64 字节。在谈论缓存时，谈论行大小或缓存行是很常见的事，它指的是镜像主内存的一块。高速缓存只能以高速缓存行的倍数加载和存储内存。缓存具有自己的层次结构，通常称为 L1，L2 和 L3。L1 高速缓存是最快和最小的； L2 更大，更慢，L3 更大，更慢。

L1 缓存通常进一步分为指令缓存和数据，在引入中继的基于哈佛 Mark-1 的计算机之后被称为“哈佛架构”。拆分缓存有助于减少流水线瓶颈，因为较早的流水线阶段倾向于引用指令缓存，而较后的阶段则倾向于数据缓存。除了减少对共享资源的争用之外，为指令提供单独的缓存还允许使用指令流性质的替代实现。它们是只读的，因此不需要昂贵的片上功能（例如多端口），也不需要处理子块读取操作，因为指令流通常使用更常规大小的访问。

![Cache Associativity](https://s2.ax1x.com/2020/01/27/1nssG6.png)

在正常操作期间，处理器会不断要求高速缓存检查高速缓存中是否存储了特定的地址，因此高速缓存需要某种方法来非常快速地查找其是否存在有效行。如果可以将给定地址缓存在缓存中的任何位置，则每次进行引用以确定命中或未命中时都需要搜索每个缓存行。为了保持快速搜索，这是在高速缓存硬件中并行完成的，但是对于合理大小的高速缓存而言，搜索每个条目通常过于昂贵。因此，可以通过限制特定地址必须驻留的位置来简化缓存。这是一个权衡； 高速缓存显然比系统内存小得多，因此某些地址必须别名。如果两个彼此互为别名的地址一直在不断更新，则它们将争用缓存行。

- 直接映射的缓存（Direct mapped caches）将允许缓存行仅存在于缓存中的单个条目中。这是最简单的在硬件中实现的方法，由于两个阴影地址必须共享同一条缓存行，因此无法避免混淆现象。

- 完全关联高速缓存（Fully Associative caches）将允许高速缓存行存在于高速缓存的任何条目中。由于可以使用任何条目，因此可以避免别名问题。但是在硬件中实现非常昂贵，因为必须同时查找每个可能的位置以确定值是否在缓存中。

- 集合关联缓存（Set Associative caches）是直接关联和完全关联缓存的混合，并且允许特定的缓存值存在于缓存内的某些行子集中。高速缓存被划分为偶数部分，称为方式，并且可以以任何方式定位特定地址。因此，n 路集关联缓存将允许在行大小为 set n 的总块 mod n 中的任何条目中存在一条缓存行。上图中“缓存关联性”显示了一个示例的 8 元素，4 路集关联缓存。在这种情况下，这两个地址具有四个可能的位置，这意味着在查找时仅必须搜索一半的缓存。方式越多，可能的位置就越多，混淆现象就越少，从而导致总体上更好的性能。

一旦缓存已满，处理器就需要清除一行来为新行腾出空间，处理器可以通过多种算法选择逐出哪条线。例如，最近最少使用（LRU）是一种算法，其中丢弃最旧的未使用的行以为新行腾出空间。当仅从高速缓存中读取数据时，无需确保与主内存的一致性。但是，当处理器开始写入高速缓存行时，它需要就如何更新底层主内存做出一些决定。

- 直写式高速缓存（Write-through）将在处理器更新高速缓存时将更改直接写入主系统内存。这是较慢的，因为如我们所见，写入主存储器的过程较慢。

- 回写缓存（Write-back）会将更改延迟写入 RAM，直到绝对必要为止。明显的优点是，写入缓存条目时需要较少的主存储器访问。已写入但未提交给内存的缓存行称为脏行。缺点是，当退出缓存条目时，它可能需要两次内存访问（一次写入脏数据主内存，另一次加载新数据）。

如果一个条目同时存在于较高级别和较低级别的高速缓存中，则我们说较高级别的高速缓存是 Inclusive；否则，如果具有一行的较高级别的高速缓存消除了具有该行的较低级别的高速缓存的可能性，我们说它是排他的（Exclusive）。

## 缓存寻址

到目前为止，我们还没有讨论过缓存如何确定给定地址是否驻留在缓存中。显然，高速缓存必须保留当前驻留在高速缓存行中的数据的目录。缓存目录和数据可能位于同一处理器上，但也可能是分开的，例如在具有核心 L3 目录的 POWER5 处理器的情况下，但是实际上访问数据需要遍历 L3 总线才能访问 核心内存。这样的安排可以促进更快的命中/未命中处理，而不会产生将整个缓存保留在内核中的其他成本。

![Cache tags](https://s2.ax1x.com/2020/01/27/1n67Dg.png)

需要并行检查标签以降低等待时间。更多的标记位（即，较少的设置关联性）需要更复杂的硬件来实现。另外，更多的集合关联性意味着更少的标签，但是处理器现在需要硬件来多路复用许多集合的输出，这也可能增加延迟。

为了快速确定地址是否位于缓存中，将其分为三个部分：标签，索引和偏移量。偏移位取决于高速缓存的行大小。例如，一个 32 字节的行大小将使用地址的最后 5 位（即 25）作为行的偏移量。索引是一个条目可能驻留的特定缓存行。例如，让我们考虑一个具有 256 个条目的缓存。如果这是一个直接映射的缓存，我们知道数据可能仅驻留在一条可能的行中，因此偏移量后的下一个 8 位（28）描述了要检查的行-0 到 255 之间。

现在，考虑相同的 256 元素高速缓存，但是分为两种方式。这意味着有两组 128 行，并且给定地址可以位于这两个组中的任何一个中。因此，仅需要 7 位作为索引即可偏移到 128 个条目的路径中。对于给定的高速缓存大小，随着方法数量的增加，由于每种方法都会变小，因此减少了作为索引所需的位数。高速缓存目录仍然需要检查高速缓存中存储的特定地址是否是它感兴趣的那个地址。因此，该地址的其余位是高速缓存目录对照传入的地址标记位进行检查以确定是否存在标记位。缓存命中与否。“缓存标签”中说明了这种关系。

当存在多种方式时，此检查必须在每种方式中并行进行，然后将其结果传递到多路复用器，该多路复用器输出最终的命中或未命中结果。如上所述，高速缓存的关联性越高，索引所需的位越少，而标记位则越多-到完全关联的高速缓存的极端（其中没有位用作索引位）。标签位的并行匹配是高速缓存设计的昂贵组件，并且通常是高速缓存可以增长多少行（即，多大）的限制因素。

# 外设与总线
